{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPuzw/+XByyfGMJQDujOJUT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ‚õìÔ∏è‚Äçüí• LangChain: Models, Prompt, and Output Parsers"],"metadata":{"id":"ns6cOwnRby1P"}},{"cell_type":"markdown","source":["By the end of this notebook, you'll build a **Customer Support Classifier**\n","that takes raw support messages and outputs structured JSON.\n","\n","**What we'll cover:**\n","1. Models ‚Äî connecting to an LLM\n","2. Prompt Templates ‚Äî reusable, structured prompts\n","3. Output Parsers ‚Äî structured JSON from messy text\n","4. Chains (LCEL) ‚Äî piping it all together"],"metadata":{"id":"hXg8b1FELLax"}},{"cell_type":"markdown","source":["## üõ†Ô∏è Setup & Installation"],"metadata":{"id":"nljMYca9cQFY"}},{"cell_type":"code","source":["# Install required packages\n","!pip install -q langchain langchain-huggingface langchain-core"],"metadata":{"id":"aWQx-WSDDe-r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from google.colab import userdata\n","\n","# Option 1: Use Colab Secrets (recommended)\n","# Go to üîë icon in left sidebar ‚Üí Add secret: HF_API_TOKEN\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get(\"HF_API_TOKEN\")\n","\n","# Option 2: Paste directly (not recommended for sharing)\n","# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_your_token_here\"\n","\n","print(\"‚úÖ Token set!\")"],"metadata":{"id":"7fEg6WO8Dfjt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1Ô∏è‚É£ Your First LLM Call\n","Let's connect to a Hugging Face model and make our first call.\n","We're using the **free Inference API** ‚Äî no GPU needed."],"metadata":{"id":"MlePAO36cXZ_"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","from langchain_huggingface import HuggingFacePipeline\n","\n","# Load model locally on Colab GPU (free tier T4)\n","repo_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n","\n","tok = AutoTokenizer.from_pretrained(repo_id, use_fast=True)\n","model = AutoModelForCausalLM.from_pretrained(\n","    repo_id,\n","    torch_dtype=\"auto\",\n","    device_map=\"auto\"\n",")\n","\n","gen = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tok,\n","    max_new_tokens=512,\n","    temperature=0.1,\n","    do_sample=True,\n","    return_full_text=False\n",")\n","\n","# Wrap the pipeline for LangChain\n","llm = HuggingFacePipeline(pipeline=gen)\n","\n","print(\"‚úÖ Model loaded on\", model.device)"],"metadata":{"id":"bCVBrL56DyoK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Your first LLM call!\n","response = llm.invoke(\"Explain how coffee is made, but in 10 words and like you‚Äôre a caveman.‚Äù\")\n","print(response)"],"metadata":{"id":"NuUa3oZTECd-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### üß™ YOUR TURN ‚Äî Try different prompts"],"metadata":{"id":"fYGBUe9MGpTf"}},{"cell_type":"code","source":["# TODO: Replace the ______ with your own question and run the cell\n","your_response = llm.invoke(\"______\")\n","print(your_response)"],"metadata":{"id":"Dy5rlSsRGNd7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### üß™ YOUR TURN ‚Äî Change the temperature"],"metadata":{"id":"bEIBIADCGuvM"}},{"cell_type":"code","source":["# TODO: Create a new pipeline with temperature=0.9\n","# What changes in the output?\n","\n","gen_creative = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tok,\n","    max_new_tokens=512,\n","    temperature=______,  # TODO: set to 0.9\n","    do_sample=True,\n","    return_full_text=False\n",")\n","\n","creative_llm = HuggingFacePipeline(pipeline=gen_creative)\n","\n","prompt_text = \"Write a one-line tagline for a coffee shop.\"\n","\n","print(\"üßä Low temp (0.1):\", llm.invoke(prompt_text))\n","print(\"üî• High temp (0.9):\", creative_llm.invoke(prompt_text))"],"metadata":{"id":"INvgev17GxGx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Wrap it as a Chat Model"],"metadata":{"id":"T0j5msXJHCH9"}},{"cell_type":"code","source":["from langchain_huggingface import ChatHuggingFace\n","\n","# Wrap as chat model (supports system/human messages)\n","chat_model = ChatHuggingFace(llm=llm)\n","\n","from langchain_core.messages import HumanMessage, SystemMessage\n","\n","messages = [\n","    SystemMessage(content=\"You are a helpful assistant. Be concise.\"),\n","    HumanMessage(content=\"What is LangChain?\"),\n","]\n","\n","response = chat_model.invoke(messages)\n","print(response.content)"],"metadata":{"id":"Gf4MqlTBHF8n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### üß™ YOUR TURN ‚Äî Change the system persona"],"metadata":{"id":"3jjhyCJwHIxI"}},{"cell_type":"code","source":["# TODO: Change the SystemMessage to make the model respond\n","# as a pirate, a poet, or a sports commentator\n","\n","messages = [\n","    SystemMessage(content=\"______\"),  # TODO: Write your persona\n","    HumanMessage(content=\"Explain what an API is.\"),\n","]\n","\n","response = chat_model.invoke(messages)\n","print(response.content)"],"metadata":{"id":"kw9ZEEGkHNsk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create a prompt template"],"metadata":{"id":"c4EqsenkHQ0K"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","# Define a reusable prompt template\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"You are a customer support expert. Be concise and professional.\"),\n","    (\"human\", \"Classify this support ticket: {ticket_text}\")\n","])\n","\n","# Test it ‚Äî see what the formatted prompt looks like\n","formatted = prompt.format_messages(ticket_text=\"I was charged twice for my subscription!\")\n","for msg in formatted:\n","    print(f\"[{msg.type}]: {msg.content}\")"],"metadata":{"id":"UxyB4B95HT4d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### üß™ YOUR TURN ‚Äî Build your own template"],"metadata":{"id":"9UiVTo6zHcT5"}},{"cell_type":"code","source":["# TODO: Create a prompt template for a DIFFERENT use case\n","# Ideas: email writer, tweet generator, code explainer, recipe suggester\n","\n","my_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"______\"),           # TODO: System instruction\n","    (\"human\", \"______: {______}\")   # TODO: Human message with a variable\n","])\n","\n","# Test it\n","formatted = my_prompt.format_messages(______=\"______\")  # TODO: Fill variable\n","for msg in formatted:\n","    print(f\"[{msg.type}]: {msg.content}\")"],"metadata":{"id":"PQ06uOkEHfEV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###  üß™ YOUR TURN ‚Äî Multiple variables"],"metadata":{"id":"50Hfd6PxHrHo"}},{"cell_type":"code","source":["# TODO: Create a template that uses TWO variables\n","# Example: \"Write a {tone} email to {recipient} about ...\"\n","\n","multi_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"You are a professional email writer.\"),\n","    (\"human\", \"Write a {______} email to {______} about a project delay.\")  # TODO: Add 2 variables\n","])\n","\n","formatted = multi_prompt.format_messages(______=\"______\", ______=\"______\")  # TODO: Fill both\n","for msg in formatted:\n","    print(f\"[{msg.type}]: {msg.content}\")"],"metadata":{"id":"PR75ONjuHtvd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define the Pydantic schema"],"metadata":{"id":"29HRm8clHxZ3"}},{"cell_type":"code","source":["from pydantic import BaseModel, Field\n","\n","class TicketClassification(BaseModel):\n","    \"\"\"Schema for classifying customer support tickets.\"\"\"\n","    category: str = Field(description=\"Category: Billing, Technical, Account, or General\")\n","    urgency: str = Field(description=\"Urgency level: Low, Medium, High, or Critical\")\n","    sentiment: str = Field(description=\"Customer sentiment: Positive, Neutral, Negative, or Angry\")\n","    summary: str = Field(description=\"One-line summary of the issue\")\n","    suggested_action: str = Field(description=\"Recommended next step for the support team\")\n","\n","print(\"‚úÖ Schema defined!\")\n","print(\"Fields:\", list(TicketClassification.model_fields.keys()))"],"metadata":{"id":"xJHRzulgIra5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### üß™ YOUR TURN ‚Äî Add a new field"],"metadata":{"id":"AiWFN_eAIuyw"}},{"cell_type":"code","source":["# TODO: Add a new field to the schema. Ideas:\n","# - department: str (which team should handle this?)\n","# - is_escalation: bool (does this need a manager?)\n","# - estimated_response_time: str (how fast should we respond?)\n","\n","class TicketClassificationV2(BaseModel):\n","    \"\"\"Extended schema with your custom field.\"\"\"\n","    category: str = Field(description=\"Category: Billing, Technical, Account, or General\")\n","    urgency: str = Field(description=\"Urgency level: Low, Medium, High, or Critical\")\n","    sentiment: str = Field(description=\"Customer sentiment: Positive, Neutral, Negative, or Angry\")\n","    summary: str = Field(description=\"One-line summary of the issue\")\n","    suggested_action: str = Field(description=\"Recommended next step for the support team\")\n","    ______: ______ = Field(description=\"______\")  # TODO: Add your field\n","\n","print(\"Fields:\", list(TicketClassificationV2.model_fields.keys()))"],"metadata":{"id":"xldp9_z4IxSh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create the output parser"],"metadata":{"id":"NpSY-gEyI2vW"}},{"cell_type":"code","source":["from langchain_core.output_parsers import PydanticOutputParser\n","\n","# Create a parser from our schema\n","parser = PydanticOutputParser(pydantic_object=TicketClassification)\n","\n","# See what instructions the parser generates for the LLM\n","print(parser.get_format_instructions())"],"metadata":{"id":"NRYeEl5uI5OT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ü§î Discussion Question\n","Look at the format instructions printed above.\n","\n","1. What format is the LLM being asked to return?\n","2. Why does the parser need to tell the LLM *how* to format its response?\n","3. What would happen if we skipped the parser and just used raw LLM text?"],"metadata":{"id":"68YocRDfI8nf"}},{"cell_type":"markdown","source":["### Build the full prompt with parser instructions"],"metadata":{"id":"SJnau8m8I99M"}},{"cell_type":"code","source":["prompt = ChatPromptTemplate.from_messages([\n","    (\"system\",\n","     \"You are a customer support classifier. \"\n","     \"Analyze the support ticket and classify it.\\n\\n\"\n","     \"{format_instructions}\"),\n","    (\"human\", \"{ticket_text}\")\n","])\n","\n","formatted = prompt.format_messages(\n","    format_instructions=parser.get_format_instructions(),\n","    ticket_text=\"I was charged twice!\"\n",")\n","print(formatted[0].content[:300], \"...\")"],"metadata":{"id":"hHW98b8uJE5J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Build the chain"],"metadata":{"id":"j_IFhtiGJH5Q"}},{"cell_type":"code","source":["# Build the chain: prompt ‚Üí model ‚Üí parser\n","chain = prompt | chat_model | parser\n","\n","print(\"‚úÖ Chain built!\")\n","print(\"Pipeline: prompt ‚Üí chat_model ‚Üí parser\")"],"metadata":{"id":"vLo1pODXJKAF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### üß™ YOUR TURN ‚Äî Build the chain yourself"],"metadata":{"id":"ilSJjyjKJPIa"}},{"cell_type":"code","source":["# TODO: Without looking above, build the chain from scratch\n","# Fill in the three components in the right order\n","\n","my_chain = ______ | ______ | ______  # TODO: prompt, chat_model, or parser?\n","\n","# Test it\n","test_result = my_chain.invoke({\n","    \"ticket_text\": \"Your app keeps crashing on my iPhone.\",\n","    \"format_instructions\": parser.get_format_instructions()\n","})\n","\n","print(f\"Category: {test_result.category}\")\n","print(f\"Urgency:  {test_result.urgency}\")"],"metadata":{"id":"KBGDpCtTJRRl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Run the chain"],"metadata":{"id":"T-tlSpBFJXDw"}},{"cell_type":"code","source":["result = chain.invoke({\n","    \"ticket_text\": \"I've been charged twice for my subscription this month and nobody is responding to my emails. I want a refund NOW or I'm canceling.\",\n","    \"format_instructions\": parser.get_format_instructions()\n","})\n","\n","print(f\"Category:         {result.category}\")\n","print(f\"Urgency:          {result.urgency}\")\n","print(f\"Sentiment:        {result.sentiment}\")\n","print(f\"Summary:          {result.summary}\")\n","print(f\"Suggested Action: {result.suggested_action}\")"],"metadata":{"id":"CzRlh5hwJYe5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### See it as JSON"],"metadata":{"id":"_3gw_FCqJbNk"}},{"cell_type":"code","source":["import json\n","print(json.dumps(result.model_dump(), indent=2))"],"metadata":{"id":"Ee1GZOjSJdte"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Test on multiple tickets"],"metadata":{"id":"bv9_lfF8JfWF"}},{"cell_type":"code","source":["test_tickets = [\n","    \"My login isn't working and I have a presentation in 10 minutes!\",\n","    \"Hey, just wanted to say your product is fantastic. Keep it up!\",\n","    \"How do I change my subscription plan? I can't find the option.\",\n","    \"This is the THIRD time my order has been wrong. I want to speak to a manager.\",\n","    \"Can you help me integrate your API with my Python project?\",\n","]\n","\n","print(\"=\" * 60)\n","for i, ticket in enumerate(test_tickets, 1):\n","    print(f\"\\nüé´ Ticket {i}: {ticket}\\n\")\n","    try:\n","        result = chain.invoke({\n","            \"ticket_text\": ticket,\n","            \"format_instructions\": parser.get_format_instructions()\n","        })\n","        print(f\"   Category:  {result.category}\")\n","        print(f\"   Urgency:   {result.urgency}\")\n","        print(f\"   Sentiment: {result.sentiment}\")\n","        print(f\"   Summary:   {result.summary}\")\n","        print(f\"   Action:    {result.suggested_action}\")\n","    except Exception as e:\n","        print(f\"   ‚ùå Error: {e}\")\n","    print(\"-\" * 60)"],"metadata":{"id":"VQ34sS2_JiHH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### üß™ YOUR TURN ‚Äî Add your own tickets"],"metadata":{"id":"lKCeajXcK0ki"}},{"cell_type":"code","source":["# TODO: Add 3 of your own support tickets to this list\n","my_tickets = [\n","    \"______\",  # TODO: Write ticket 1\n","    \"______\",  # TODO: Write ticket 2\n","    \"______\",  # TODO: Write ticket 3\n","]\n","\n","for i, ticket in enumerate(my_tickets, 1):\n","    print(f\"\\nüé´ My Ticket {i}: {ticket}\\n\")\n","    result = chain.invoke({\n","        \"ticket_text\": ticket,\n","        \"format_instructions\": parser.get_format_instructions()\n","    })\n","    print(json.dumps(result.model_dump(), indent=2))\n","    print(\"-\" * 60)"],"metadata":{"id":"zh0S8R92K1d5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Take Home: üèÜ CHALLENGE ‚Äî Build a completely different classifier"],"metadata":{"id":"qp3bxcHTK8xF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"prGCWLa9bklK"},"outputs":[],"source":["# üèÜ CHALLENGE: Build your OWN classifier from scratch!\n","# Pick one:\n","#   - Movie review ‚Üí genre, rating, mood, one-line-summary\n","#   - Job posting ‚Üí role_level, department, remote_or_onsite, key_skills\n","#   - Food review ‚Üí cuisine, price_range, would_recommend, highlights\n","#\n","# Steps:\n","#   1. Define a Pydantic schema\n","#   2. Create a parser\n","#   3. Write a prompt template\n","#   4. Build the chain\n","#   5. Test it!\n","\n","# Step 1: Define your schema\n","class ______(BaseModel):\n","    \"\"\"______\"\"\"\n","    ______: str = Field(description=\"______\")\n","    ______: str = Field(description=\"______\")\n","    ______: str = Field(description=\"______\")\n","    ______: str = Field(description=\"______\")\n","\n","# Step 2: Create the parser\n","my_parser = PydanticOutputParser(pydantic_object=______)\n","\n","# Step 3: Write the prompt\n","my_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"______\\n\\n{format_instructions}\"),\n","    (\"human\", \"{______}\")\n","])\n","\n","# Step 4: Build the chain\n","my_chain = ______ | ______ | ______\n","\n","# Step 5: Test it!\n","my_result = my_chain.invoke({\n","    \"______\": \"______\",\n","    \"format_instructions\": my_parser.get_format_instructions()\n","})\n","\n","print(json.dumps(my_result.model_dump(), indent=2))"]},{"cell_type":"markdown","source":["## üéØ Recap"],"metadata":{"id":"8zMS_MzgeN4a"}},{"cell_type":"markdown","source":["### What You Learned\n","\n","#### 1. Models\n","You connected to an open-source LLM through Hugging Face's free Inference API using `HuggingFaceEndpoint` and `ChatHuggingFace`. You saw that LangChain is model-agnostic ‚Äî swap one line and your entire pipeline works with a different model.\n","\n","#### 2. Prompt Templates\n","You replaced messy f-strings with `ChatPromptTemplate` ‚Äî reusable, version-able, and variable-injected. You built templates with single and multiple variables, and experimented with system personas.\n","\n","#### 3. Output Parsers\n","You defined a `Pydantic` schema and used `PydanticOutputParser` to force the LLM to return structured JSON instead of raw text. This is what separates a demo from a production app.\n","\n","#### 4. Chains (LCEL)\n","You piped everything together with the `|` operator:\n","\n","```python\n","chain = prompt | chat_model | parser\n","```\n","\n","Three pipes. Three transformations. One line of code."],"metadata":{"id":"U-7k4N0YL5nE"}}]}